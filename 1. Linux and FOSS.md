# Linux and FOSS


## Introduction

Have you ever watched a movie on Netflix? Bought an item on Amazon or eBay? Posted on Facebook? Or stored any of your documents in the cloud? Then you have definitely used Linux-based operating systems. 

A comprehensive understanding of Linux and its history is fairly fundamental in your journey down the Bitcoin rabbit hole. Both are products of the free and open source software (FOSS) movement, so their usage is, to some extent, motivated by similar values (e.g., privacy, security, freedom). The same can be said for the working norms and standards with regards to development of these projects. On a practical level, working with Linux for your Bitcoin applications—rather than operating systems from the Windows or Apple families—will harden your security and privacy.     

So what exactly is Linux? What does it mean to say that that it is free and open source? And why you might want to use Linux? These are the main questions, I tackle in this primer. 

I will begin, however, with a clarification of the term “server,” as we will come across it frequently in our discussion here (as well as in other primers). 


## Servers and clients

The term server can have a number of interrelated meanings.   

To start, a **server** can refer to any computer that predominantly provides resources to a network. It is in this case, therefore, the name of a *functional role* that a computer takes in a network. A server is usually contrasted with a **client**, which refers in this context to a computer that predominantly requests resources from a network. 

In an office network you might have a file server. Through their own computers, employees can store and request files from the server as needed for their tasks. 

On the internet, **web servers** store all the relevant files for web sites, which users can request to be displayed in their browsers. This collection of content is also known as the **World Wide Web** (importantly, we can also communicate via other types of protocols on the internet, so the internet is not just about the World Wide Web). 

As servers can vary widely in their network functions, you will see them with many different labels: general purpose servers, application servers, authentication servers, data servers, printer servers, domain name servers, Bitcoin servers, and so on. All these have in common that they are computers which predominantly provide resources to the computers of end users. 

There is a second way that term **server** is often used. Typically file servers, web servers, and other such functional roles in a network are the responsibility of specialized hardware. This type of hardware has very high quality components and is less subject to failure than regular personal computers or laptops. 

Amazon, for example, has rows and rows of such specialized hardware in large warehouses to service the customers to their online store. Given that such specialized devices are intended to be used as servers in the functional sense, they are typically also called servers or server hardware. 

Strictly speaking, not every server in the functional sense needs to be a server in the hardware sense. You can also set up a regular desktop or laptop to function as a server in a network, though these are not servers in the hardware sense. 

One of the core features of decent Bitcoin node software—foremost Bitcoin Core—is that you really do not need server hardware to run it. At least many desktops, laptops, and cheap, single-board computers such as the Raspberry Pi are also up to that task. This is instrumental to keeping the Bitcoin Network decentralized. 

A final and third usage of the term **server** is to denote a program that only services requests from other programs. Users do not interact with such programs directly, but through a **client** or client application. 

For instance, the main command-line server program in Bitcoin Core is called bitcoind. You never interact with that program directly, but through a command line application called bitcoin-cli. These kinds of server applications can run on servers in the functional sense, but they also run on our personal computers. 


## What is Linux?

**Linux** is a family of operating systems, one that is much less centralized than the Windows family of operating systems. 

Popular desktop examples of **Linux distributions** are Debian, Ubuntu, Fedora, Red Hat Enterprise Linux, and CentOS. Android for mobile phones is arguably also a Linux distribution. Raspbian is a Linux distribution based on Debian that was developed specifically for the Raspberry Pi computer. There are hundreds of other Linux distributions.<sup>[1](#footnote1)</sup>

So when I say that you are using Linux when you watch a movie on Netflix, buy an item on Amazon or Ebay, post on Facebook, or store documents in the cloud, what I really mean is that you are using servers with Linux distributions on them.  

While Linux might not be a popular choice for an operating system on a personal computer, it is an incredibly popular choice for web servers. According to research from W3Techs, probably around 70% of servers use Unix or Unix-like operating systems, while only around 30% seem to use a Windows operating system. Of the Unix and Unix-like operating systems, the majority would be Linux distributions.<sup>[2](#footnote2)</sup>

The Linux distributions on servers would be very similar to what you would run on a desktop, though many distributions do have versions that are specifically tailored for server usage (e.g., Fedora versus Fedora Server). 

The popularity of Linux for web servers is evident from the concept of a LAMP configuration, which means a web server that includes a Linux distribution, Apache (the web server in the sense of the software), MySQL (database software), and PHP (a computer language for any code executed on the web server rather than in your browser, therefore, also known as a server side language). If you have ever created a website and used a hosting service, it is highly likely that your web server would have come with a LAMP configuration. 

So what exactly unites the hundreds of different Linux distributions? The common way that all these distributions are linked is through their use of the Linux Kernel. To understand that statement a little better, we need to dig into our operating systems a little. 

The two primary functions for the operating systems on our computers are abstraction and arbitration.<sup>[3](#footnote3)</sup>

With regards to the former, the operating system provides a common interface for our hardware, even though at a low level the hardware parts work differently and require different instructions; that is, the operating system abstracts away from the low-level details for applications. 

This means that we do not have to program each application on our computer to work with our specific hardware devices, nor that we have to reprogram our applications when the hardware configuration is changed. It is this abstraction by the operating system that allows us to have a large variety of hardware devices on the market. 

Second, the operating system arbitrates the usage of hardware resources by applications, including access to the CPU, RAM, and storage disks. All requests by applications for hardware resources run through the operating system, while hardware, in turn, sends all information to applications via the operating system. The operating system ensures that multiple applications can use the hardware simultaneously.  

These two primary functions of an operating system, abstraction and arbitration, are handled by its kernel. 

The kernel is a very important part, but not the only part, of the operating system (though people sometimes do define the operating system as the kernel). To clarify this point, we can distinguish three layers of software programs for our computer systems: (1) the core, (2) the middle layer, and (3) the application layer. 

Beginning with the outermost layer, **applications** are used to perform activities that are of direct benefit to the end-user. These can include word-processing applications, accounting applications, games, web browsers, e-mail applications, and so on. 

The middle layer covers libraries, utilities, and compilers/interpreters. 

**Libraries** are sets of code that can be utilized in running and building applications. Libraries allow programmers to make applications much more efficiently. 

**Utilities** are software programs that help the user manage and monitor their computer. These can include anti-virus programs, partition editors, and disk defragmenters. 

Finally, **interpreters** and **compilers** help translate source code—such Python, Rust, and C—into executable instructions for our hardware. (For more discussion on interpreters and compilers, see Section 2 of Primer 2, which covers command line control). 

Libraries, utilities, and interpreters/compilers make up the middle layer of your computer system. The core application of your computer system—which performs the main role of abstraction and arbitration—is then the kernel. 

The kernel is a key application for any operating system. But any operating system will also come with various middle-layer programs and end-user applications. A Windows operating system, for example, will also come with programs such as Notepad, MS Paint, and Disk Management, but these are not part of the Windows kernel. 

Upon loading the operating system, your computer memory will be divided into two sections: user space and kernel space. The kernel will be closed off and executing in kernel space, while the rest of the operating system and various other software programs will run in the user space. 

We generally draw a distinction between two types of kernels. If the kernel only contains the minimal code for ensuring abstraction and arbitration, it is usually known as a **micro-kernel**. With a micro-kernel you will have a small core program in your kernel space and many of the operating system’s services, such as device drivers, actually sitting in user space. For many of the tasks of the operating system, therefore, the kernel will communicate with programs placed in user space. 

This type of micro-kernel is commonly contrasted with a **monolithic kernel**. This type of kernel is a much larger program that contains all the major operating system components. It contains much more than just the minimal components necessary for abstraction and arbitration. 

Going back to what unites all the different Linux distributions, it is their common use of the **Linux kernel**. The kernel was originally created by Linus Torvalds and released in 1991. The term Linux is actually combination of Linus and **Unix**, an operating system developed in AT&T’s Bell Labs in the 1970s.<sup>[4](#footnote4)</sup> Torvalds’ idea was to create an open source operating system that was Unix-like: similar to Unix but without the licensing requirements.<sup>[5](#footnote5)</sup>

Most modern operating systems, including those for Apple products, have their roots in Unix. The primary exception is Microsoft Windows. 

The Linux kernel is generally regarded as a monolithic kernel (though no operating system, including any Linux distributions, falls exactly into these academic categories).  

For the sake of clarity, when we say that all Linux distributions leverage the Linux kernel, it does not mean that all their versions of the kernel are exactly the same. When Torvalds released his kernel in 1991, he did so as an open source project. Since then, it has seen many different releases and new versions.<sup>[6](#footnote6)</sup>

Any particular Linux distribution may vary with regards to the specific kernel version and the release they employ. Many distributions have their own customizations within the kernel for their own purposes (and, of course, individuals can even tweak the kernel according to their own requirements with sufficient technical knowledge). 

Hence, when we say that all Linux distributions are using the Linux kernel, we mean that they are all using very similar, but not necessarily exactly identical kernels. 


## Free and open source software

The Linux kernel is **free and open source software (FOSS)**. Many would argue that the Linux kernel is the most successful FOSS project in the history of software development. It also had a massive impact on popularizing the free and open source software movement among consumers and corporations. So to understand Linux, you really need to know something about FOSS. 

To understand free and open source software, we have to go back to 1984 when Richard Stallman started the **GNU project**, a project which is still active today. Repeatedly running into software usage constraints, his mission with the project was to develop a Unix-like operating system that was not subject to such constraints. This operating system was to be called GNU, which is a recursive acronym meaning “GNU’s not Unix.” 

To describe the operating system he envisioned, Stallman coined the term **free software**. The term “free” here roughly means that users have the freedom “to run, copy, distribute, study, change and improve the software.”<sup>[7](#footnote7)</sup> Hence, it refers to the political and ethical notion of freedom to describe a situation in which users are empowered and in control. The term does not refer to the price of the software—“free” software in this sense is better called freeware—and there is no rule that you cannot commercialize free software in Stallman’s sense.<sup>[8](#footnote8)</sup>

According to the website of the GNU project, a software program can be considered free if you as a user have the following four essential freedoms:<sup>[9](#footnote9)</sup>

1. “The freedom the run the program as you wish, for any purpose.”
2. “The freedom to study how the program works, and change it so it does your computing as you wish…. Access to the source code is a precondition for this.”
3. “The freedom to redistribute copies so you can help others.”
4. “The freedom to distribute copies of your modified versions to others…. By doing this you can give the whole community a chance to benefit from your changes. Access to the source code is a precondition for this.” 

Any software that is not free and where the program and its developers control the users is proprietary according to Stallman’s understanding. 

Importantly, Stallman’s idea of free software was not as revolutionary in the 1980s as it might sound. The idea of closing up software via restrictions only really caught on in the late 1970s, much of it due to the work of Microsoft. 

In a now infamous letter to the Homebrew Computer Club in 1976 entitled “An open letter to hobbyists,” Bill gates argued that software development was being impeded by “theft,” and that many members of the computer club were guilty of this themselves. In his view, this theft was significantly impeding quality software development. Hence, Stallman’s notion of free software and the start of the GNU project was a response to recent developments at the time.  

The four main freedoms Stallman emphasizes are conceptualized in the **GNU General Public License**.<sup>[10](#footnote10)</sup>  Any software program that meets its standards can receive the license and consider itself free software. The GNU General Public License is a **copyleft license** (as distinguished from a copyright license): it allows anyone to freely use the software, as long as any derivative works are distributed to users with the same rights. 

The Linux kernel is protected by the GNU General Public License. Bitcoin, on the other hand, is protected by the **MIT License**. This is what is known as a **permissive software license**, which does not come with the strong restrictions on redistribution contained within the GNU General Public License. Most importantly, you can re-license modified versions of the Bitcoin software as proprietary software.  

Importantly, many Linux distributions are more accurately GNU/Linux distributions. As mentioned earlier, an operating system is not just its kernel, but also includes libraries, utilities, compilers/interpreters, and basic software applications. And most distributions with a Linux kernel rely heavily on free software from the GNU project for the rest of the operating system, such as the GNU C Compiler and the GNU C Library. 

A notable exception to all of this is the Android operating system. While it relies on the Linux kernel, Android does not rely on GNU software.  

Now let’s turn to the concept of **open source software**. A colloquial understanding of open source software is as a software program for which anyone can explore and verify the source code. It is arguably a necessary condition of free software that it is open source in this sense, but the reverse is surely not true (i.e., you can have publicly open source code, yet still have a software application that is not free). 

This rather intuitive understanding of “open source” is, however, not how the originators of the term intended it. The term open source was devised during a strategy session held at the offices of VA Linux systems in Palo Alto on February 3, 1998, which included a number of company representatives as well as others. Those present had a much broader concept in mind than just source code which is open, one that is close to Stallman’s notion of free software. In fact, many of them were at least to some degree advocates of free software, but were seeking to achieve two goals with the introduction of the term open source software. 

First, the originators of the term were emphasizing a particular development process and the benefits it could have for the quality of software. Although this development style largely seems to fit the ethos of free software, the use of the term open source emphasizes the results of that process rather than any political/moral notion of freedom. 

Second, while the originators of the term realized the connection to free software (many indeed were supporters to some degree), they believed the term open source was much more marketable in the corporate sector. The term free software was often understood as freeware at the time in executive circles—it still is—and also had a number of negative connotations. 

That the originators of the term open source were much more concerned with the outcome of a particular style of development, rather than any intrinsic value of freedom that could be associated with that style, is seen in a seminal essay from one of the key people in that meeting, Eric Raymond. The essay was entitled “The Cathedral and the Bazaar,” and it was later also turned into a book.<sup>[11](#footnote11)</sup>

In his essay and the subsequent book, Raymond explores the principles that can make community-style projects such as on the Linux kernel a success. The development of Linux was rather contrary to many accepted principles at the time. Accepted practice, certainly for complex projects, was a top-down closed approach which he termed the **cathedral style** of software development. The Linux community, instead, was a “great babbling bazaar of differing agendas and approaches (aptly symbolized by the Linux archive sites, which would take submissions from anyone) out of which a coherent and stable system could seemingly emerge only by a succession of miracles.”<sup>[12](#footnote12)</sup>

How is it that projects with this **bazaar style** of software development can become a success? Though Raymond gives many principles, probably the most important principle is summarized as follows: Given enough eyeballs, all bugs are shallow.”<sup>[13](#footnote13)</sup> It is this wisdom of the crowds type principle that gave Linux its most significant success. 

Importantly, Raymond was much less concerned with any notion of freedom associated with this style of software development. As he notes, 

> Perhaps in the end the open-source culture will triumph not because cooperation is morally right or software 'hoarding' is morally wrong (assuming you believe the latter, which neither Linus nor I do), but simply because the closed-source world cannot win an evolutionary arms race with open-source communities that can put orders of magnitude more skill time into a problem."<sup>[14](#footnote14)</sup>

Shortly after the strategy session at VA Linux Systems, Eric Raymond and Bruce Perens founded the **Open Source Initiative**. One of the first points of order was to draft the **Open Source Definition**, a document that outlines when a software program meets the standards for open source, which was based on the *Debian Free Software Guidelines* also written by Bruce Perens. 

You can read the current version of the definition on the organization’s website. Unlike the GNU Public License, the open source certification is nowadays given to licenses, rather than software applications.<sup>[15](#footnote15)</sup> The MIT License, which governs Bitcoin, has this certification, as does the GNU General Public License. 

So what then are we to make of this distinction between free software and open source software? 

Just think of them as two broad ways to categorize our software. Labeling any software as free emphasizes the value of the power and freedom that users have with regards to that software. It also suggests that this freedom has substantial value independently of how it impacts the quality of the software. 

If you describe software as open source, you are instead more describing the process by which the software is created. Many of the early proponents of open source software valued this process not so much for any intrinsic reasons, but rather because of the quality of the software that it could produce. As is seen for both Linux and Bitcoin, the level of quality and innovation in a community-driven open source project can indeed be dazzling. 

The terms free software and open source software are rather broad terms, and could be conceptualized in different ways. Any specific conceptualization is likely to remain contested and changeable, rather than universally accepted. Otherwise, the GNU project and the Open Source Initiative would never have to update their licensing terms. Nevertheless, specific conceptualizations of these two terms are likely to overlap significantly. 

You can see this by comparing the understanding of free software as currently propounded by the GNU project and the understanding of open source as currently suggested by the Open Source Initiative. Most software programs that meet the standard for free software would meet that for open source software according to these specific conceptualizations, and vice versa. 

As Stallman explains,<sup>[16](#footnote16)</sup> however, there are probably some exceptions such as the Android operating system: this could be considered open source software according to the Open Source Initiative, but not free software as defined by the GNU project.<sup>[17](#footnote17)</sup>  


## FOSS collaboration

Since its first release on September 17, 1991, over 20,000 people have made code contributions to the Linux kernel.<sup>[18](#footnote18)</sup> Since its first release on September 17, 1991, over 20,000 people have made code contributions to the Linux kernel.<sup>[19](#footnote19)</sup> While most FOSS projects are run on a smaller scale, any such project requires good tools to enable collaboration.

It is good to have some understanding of two primary tools which FOSS developers (as well as developers on non-FOSS projects) commonly use: version control systems and repository hosting services. 

**Version control systems** offer developers a system of record-keeping for software projects. In addition, they allow for making changes to software projects in a systematic fashion. Overall, version control systems provide developers a basis for collaboration. 
 
The command-line application **Git** is probably the most popular version control system, certainly for FOSS development. It is used both in the development of Bitcoin Core and the Linux kernel. 

Git was developed in 2005 by Linus Torvalds with the help of various contributors to enable decentralized collaboration on the Linux kernel. Their efforts were the motivated by the imposition of licensing requirements on Bitkeeper, the proprietary version control system used to manage the Linux kernel before Git. 

Using Git for a project, starts with creating a **Git repository** (or **Git repo**). Unlike a software repository—which typically refers to a storage location for a collection of software packages (e.g., a Debian repository)—a Git repository is typically a storage location for a single software package. 

A “storage location” here is literally no more than a directory with the subdirectories and files that belong to the software project. This includes the source code of one or more applications included in the software package. It also normally includes additional information about the software package, such as installation instructions and licensing information. 

You can see whether any directory is coupled to Git—and, thus, a Git repository—by the presence of a “.git” subdirectory.<sup>[20](#footnote20)</sup>

A **repository hosting service** is a type of online platform that developers can use to host repositories tracked by version control systems. Typically these platforms offer a much friendlier interface to version control systems with additional tools and features. 

**Github** is a very popular online hosting service for Git repositories, particularly for FOSS projects. While the Bitcoin Core project was originally hosted on Sourceforge, the main repository is now on Github.<sup>[21](#footnote21)</sup> The main source of the various Linux kernel repositories—including the one still hosted by Linux Torvalds—is a website hosted by the Linux Foundation (www.kernel.org). But many of these repositories are mirrored on Github: that is, a copy of the original repository which stays synchronized automatically is hosted on Github. 

While version control systems and repository hosting services are typically used to collaborate on software projects, their usage is not actually limited to software projects. For example, Andreas Antonopoulos’ technical primer on Bitcoin, Mastering Bitcoin, is also hosted on Github and anyone is free to suggest him changes and additions.<sup>[22](#footnote22)</sup>

From here on, I will just refer to all the content of a repository tracked by a version control system as “source code”, even if it is not source code in the narrow source of the term (that is, as the source code for an application). 

Given the popularity of specifically Git to FOSS projects, it is important to have some understanding of how it works. Grasping the details requires a fairly involved educational journey. We will not venture down that path here.<sup>[23](#footnote23)</sup> However, I will briefly introduce the main concepts and offer some insights into the typical workflow for FOSS projects. 

To start, Git allows for creating save points of a software project via **commits**. A commit is, roughly speaking, a snap shot of a Git repository at a certain point in time.<sup>[24](#footnote24)</sup> Anytime you add new files to your repository or change some of them, you can create a new save point via a commit. 

Each commit is identified by a commit hash: that is, by a SHA-1 hash (160 bits) over the data in the repository, as well as a few items of metadata such as a commit message, the author of the commit, and the date of the commit. 

A Git repository is usually not just one single historical timeline of commits. Instead, it will typically have multiple **branches**. Any such branch contains its own version of the evolution of the source code in a repository via commits. While the timelines of two branches may coincide, this is not necessarily the case. 

A new branch is always created of a commit point from an existing branch. The terms **parent branch** and **child branch** are sometimes used to, respectively, indicate the original and new branches. 

A child branch will have the same history as its parent branch—either the main branch or another side branch—until the commit point at which they were split. Afterwards, any commits to the parent branch and the child branch start to occur independently of each other. The views on the evolution of your repository in these branches, thus, start to diverge. 

But while any commits on branches occur independently, you can **merge** the commits made on one branch into the history of another branch. This process of merging two branches can take on a variety of specific forms. But typically you would just attempt to merge a child branch into a parent branch, which can transpire via three main scenarios:     

1. You made commitments on the child branch, but none on the parent branch since the split of the two. The merge occurs by simply adding all the commits of the child branch to the parent branch. 
2. You made changes to both the parent and child branches, but all these changes occurred on different lines of source code. Then git can automatically incorporate the commits from the child branch into the parent branch. 
3. You made changes to both the parent and child branches, and some of these changes occurred on the same lines of source code. In this case, you have a **merge conflict** and need to make choices about what the parent branch will look like with respect to those lines that were changed in both timelines. 

The type of scenario described in (1) is called a **fast-forward merge**, as one branch just has to catch up with another. After a fast-forward merge, the histories of two branches will be identical. If at this point you make new commits to either branch, however, their views on the evolution of your source code will start to diverge again.

The type of scenario described in (2) and (3) is referred to as a **recursive merge** or a **three-way merge**. After a recursive merge, the two branches will not have an identical view on the evolution of your repository (whether a merge conflict arises or not). 

It is also possible, though not so common, to merge more than two branches at the same time in Git. This is called an **octopus merge**. 

The original branch of your software project is typically called the **main branch** or **master branch**.<sup>[25](#footnote25)</sup> All the other branches are normally called **side branches**. You can create as many side branches as needed off any commit on the main branch, and even create side branches from the commit points on other side branches. 

Branching is a key feature to implementing changes to a software project in an orderly fashion via Git. The main branch typically serves as a primary code base: that is, for production code which has been tested and examined. Side branches are, then, created to write, examine, and test source code changes. 

After sufficient development, side branches can be merged into the main branch, or into some key side branch—for instance, one that collects all the development work on some part of the software package. If the development work in a side branch is deemed unfit for the main code base, it can also be abandoned.  

After a decision to merge or abandon a side branch has been made, the branch is usually just deleted. As you typically have multiple branches contributing changes to the core code base, it is better to delete branches after they have merged. You can, then, create new branches for any other bugs or features you want to develop.  

Once sufficient changes have occurred on the main branch, developers can issue new releases of the source code. Users can download, build, and install the software application(s) from this source code. Typically developers will also release pre-compiled binaries for any release.   

Unless you are working on a software project on your own or are the sole maintainer of some project, any Git repository will typically be configured so that branch merging must happen via **pull requests**—that is, requests to merge the commits of a development branch into the main branch or some other key branch. 

Governance procedures and quality controls for software projects are implemented primarily around these pull requests. That is particularly true for FOSS development. 

Given the open nature of FOSS projects, anyone can contribute. So you have to count also on people that suggest changes with unacceptable code quality or undesirable features. You even have to count on people that might intentionally attempt to subvert the project via malicious code insertions. The governance structure and quality controls around pull requests can manage those realities for a FOSS project. 

A last important aspect to understand about Git is its distributed and decentralized nature. All the activities described above are generally not taking place in one copy of the Git repository. That would constitute a central point of failure, precisely the type of problem Git was created to solve. 

We cannot delve into all the possibilities for working with Git repositories here. But I would say that the following workflow for a contributor is fairly typical on FOSS projects: 

* The owners of the project host the Git repository online, where the code can be freely viewed by the public. Typically that repository is on Github. 
* While the contributor can view the code, she, however, cannot make changes to it directly. She cannot, for instance, just create a development branch in the repository, make some code changes, and then create a pull request. That type of access to the original repository for here and other potential contributors would likely lead to chaos. 
* Instead, the contributor can make a **fork** of the Git repository: that is, she can make a complete copy of it in some other location. Typically, she will create a fork on her own Github site, and then clone a copy of it to her local computer. While a fork is a copy of a repository that ignores any changes made to the source repository, a **clone** tracks the differences between the source and copy. This way the contributor can keep her local version and that on her own Github site synchronized. 
* After creating, examining, and testing her changes, the contributor will then create a pull request from a branch on her own Github site into the main branch or some other key branch of the original repository. 
* Sometimes her branch will be accepted or rejected straight away. Typically, her branch will first be subject to discussions and alterations to resolve any merge conflicts and other issues, before a decision is made. Sometimes this process also includes a voting procedure.  
* After a decision has been made, the contributor will usually delete the fork of the code and her local clone. If she wants to contribute a later point, she can just start the process over again. 

Git gives FOSS projects a lot of flexibility in freedom for how to organize exactly. They will tend to have certain guidelines for how to make contributions available within the repository.<sup>[26](#footnote26)</sup>


## Why use Linux?

As already suggested, you almost certainly have used Linux at some point. If you have ever had an Android phone, if you have ever made an online purchase from Amazon, eBay, or Wall Mart, if you have ever had any document backups in the cloud, or if you have ever read a news article on the World Wide Web, then you have certainly used Linux.  

Why is Linux used so much for servers? And should you be using it for your own desktop or laptop computers? If you’re a basic computer user, it is likely that you are running Windows, or perhaps a version of MacOS if you have an apple computer. What might a Linux distribution offer you that these operating systems do not? 

Most Linux distributions are free and open source in nature. The Linux kernel is governed by the GNU General Public License. Most of the various Linux distributions complement that kernel with predominantly free and open source software, often from the GNU project. Debian, for example, only includes software in their main distribution that meets the Debian Free Software Guidelines.<sup>[27](#footnote27)</sup> By contrast, Windows and MacOS are much more closed systems.  

Now there is certainly controversy over how well a Linux distribution might serve you instead of Windows or MacOS in certain cases. A lot of the question of what operating system might serve you best for a particular computer depends on who will be using that computer and for what purposes. So I hesitate to state that Linux is always better than Windows or MacOS. 

That said, I would say that at least for the Linux distributions which have a very free and open source nature, some of the main general advantages of using them are as follows:

* They will better protect your privacy. You are much less likely to be violated through tracking and monitoring in a Linux distribution like Debian, for example, than in Windows or MacOS. This, of course, does not mean you no longer have to worry about privacy at all when you use a Linux distribution, only that you are better protected.
* They typically offer a high degree of stability. This at least one reason why many people favor them particularly for servers. It is no surprise that the majority of the modern server market as well as almost all of the world’s most powerful supercomputers use Linux operating systems.   
* They commonly offer a high degree of security. Their Unix origin means they are operating systems designed to be used by multiple users (Unix was created at a time that people still generally sat behind terminals to access a central computer). Windows, by contrast, was more designed to be used by particular individuals. The free and open source nature of many Linux distributions also ensures that vulnerabilities can be quickly spotted and fixed by a large community. You are not reliant on the actions of a single company.  
* They offer more flexibility and control to the user in how their operating system works. Unlike for Windows 10, for example, you will not face a situation where it is seemingly impossible to stop automatic updates. This flexibility and control also ensures a wide variety of choices for the average user. For instance, if you like the design and feel of MacOS, there are Linux distributions such as Gmac that look very similar; all this was created by a community of enthusiasts who took advantage of the flexibility and control you have with most Linux distributions.  
* They are normally freeware, as is most of the software that you can use on them. That matters a lot, particularly if you are trying to set up simple, economical servers.

It is for many of the types of reasons above that people favor Linux distributions, both for servers as well as personal computers. Actually, we will probably also see a growing popularity of free and open source Linux distributions for your “smart” phone in the coming years such as Ubuntu Touch and SailfishOS.  

Liberal and democratic government—and freedom and privacy more specifically—is under threat from the misuse of technology. Government agencies have been actively building mass surveillance systems to track the activities of their own citizens. Technology companies can now track our movements to an extremely high degree of accuracy via our “smart” phones.

Thousands of history books can explain to you why all of this is a terrible idea. This is not a right or left issue. From a security perspective, moving to Linux-based systems is a step in the right direction.   


## Notes

<a name="footnote1">1</a>. A good source of information about the different Linux distributions is a website called DistroWatch (www.distrowatch.com). 

<a name="footnote2">2</a>. See W3Techs, “Usage Statistics of Operating Systems for Websites,” https://w3techs.com/technologies/overview/operating_system. 

<a name="footnote3">3</a>. I take this language from a course by Mike Murphy called “Operating System: Full Course.” This is available on Youtube under the following address: https://www.youtube.com/watch?v=mXw9ruZaxzQ. This is the best introduction that I have seen on operating systems, and can highly recommend it. 

<a name="footnote4">4</a>. Originally, the operating system was called UNICS (Uniplexed Information and Computing Service). No one seems to know why the acronym Unix came into circulation. 

<a name="footnote5">5</a>. I will not venture much into the history of Linux here. For a good overview, I recommend this presentation from Bryan Lunduke, “The complete history of Linux (abridged), May 7 2018 (https://www.youtube.com/watch?v=UjDQtNYxtbU), as well as the documentary “Revolution OS” (2001) (https://www.youtube.com/watch?v=vjMZssWMweA). For a comparison between Linux and Unix, see Phil Estes, “Linux vs. Unix: What’s the difference?” (May 21, 2018), available at: https://opensource.com/article/18/5/differences-between-linux-and-unix. 

<a name="footnote6">6</a>. The primary channel for releasing the source code is www.kernel.org. This website is maintained by the Linux Kernel Organization. 

<a name="footnote7">7</a>. See GNU Project, “What is free software?,” available at: https://www.gnu.org/philosophy/free-sw.html. 

<a name="footnote8">8</a>. See GNU Project, “Selling free software,” available at: https://www.gnu.org/philosophy/selling.en.html. 

<a name="footnote9">9</a>. GNU Project, “What is free software,” available at: https://www.gnu.org/philosophy/free-sw.html. 

<a name="footnote10">10</a>. GNU Project, “GNU General Public License,” available at: https://www.gnu.org/licenses/gpl-3.0.en.html. 

<a name="footnote11">11</a>. Eric Raymond, “The Cathedral and the Bazaar,” paper was presented at the Linux Kongress, Würzburg, Germany (May 27, 1997). There are a number of subsequent versions available as well as a book. My citations are from the book: Eric Raymond, *The Cathedral and the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary*, revised edn. (2001), O’Reilly: Sebastopol, CA. 

<a name="footnote12">12</a>. Ibid., pp. 21–22. 

<a name="footnote13">13</a>. Ibid., p. 30. 

<a name="footnote14">14</a>. Ibid., p. 54. 

<a name="footnote15">15</a>. Open Source Initiative, “The Open Source Definition,” available at: https://opensource.org/osd. 

<a name="footnote16">16</a>. Richard Stallman, “Is Android really free software?” The Guardian, September 19 (2011), available at: https://www.theguardian.com/technology/2011/sep/19/android-free-software-stallman. 

<a name="footnote17">17</a>.There are many different sources for this discussion on free software versus open source software. The following article by Richard Stallman is quite good: “Why open source misses the point of free software,” available at: https://www.gnu.org/philosophy/open-source-misses-the-point.html. A really good source on the history of these terms and the free and open source movement in general is the documentary “Revolution OS” (2001). You can find it here: https://www.youtube.com/watch?v=vjMZssWMweA. 

<a name="footnote18">18</a>. The Linux Foundation, “2020 Linux Kernel History Report”, August (2020), available at https://www.linuxfoundation.org/wp-content/uploads/2020_kernel_history_report_082720.pdf. 

<a name="footnote19">19</a>. Contributions in the form of “commits”. See https://github.com/bitcoin/bitcoin. 

<a name="footnote20">20</a>. At least, this is the case for any local Git repositories. Repository hosting services such as Github will have that directory operating in the background, but not displayed to you on their web interface. 

<a name="footnote21">21</a>. See, respectively, https://github.com/bitcoin/bitcoin and https://github.com/torvalds/linux. 

<a name="footnote22">22</a>. See https://github.com/bitcoinbook/bitcoinbook. 

<a name="footnote23">23</a>. I researched Git and Github tutorials rather extensively. Following are two of the best resources I could find: https://www.youtube.com/watch?v=uR6G2v_WsRA (three-part series) and https://www.youtube.com/watch?v=xAAmje1H9YM&list=PLeo1K3hjS3usJuxZZUBdjAcilgfQHkRzW. 

<a name="footnote24">24</a>. Technically, this is not exactly accurate. You first have to instruct Git to track files in the repository before they are included in commits. And sometimes you might have files that you do not want tracked by Git in the repository. Therefore, to be precise, Git creates save points of all the files that you are tracking in a depository via commits.  

<a name="footnote25">25</a>. It was traditionally commonly called the “master branch”. But in the midst of the 2020 Black Lives Matter protests, social justice warriors decided to make the term “master” with regards to software development an urgent social justice issue. Now both the terms “master” and “main” are common.  

<a name="footnote26">26</a>. For Bitcoin, you can find these here: https://github.com/bitcoin/bitcoin/blob/master/CONTRIBUTING.md. 

<a name="footnote27">27</a>. You can see the guidelines here: Debian, “Debian Free Software Guidelines,” https://www.debian.org/social_contract#guidelines. 